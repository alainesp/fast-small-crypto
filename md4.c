//////////////////////////////////////////////////////////////////////////////////////////////////////////////
// MD4 Message-Digest Algorithm
//////////////////////////////////////////////////////////////////////////////////////////////////////////////
// Automatically generated code by SIMD-function library

#include <immintrin.h>
#include <stdint.h>
#include <stdlib.h>

#define rotl32(v, s) _rotl(v, s)
#define avx2_rotl_u32(v, s) _mm256_or_si256(_mm256_slli_epi32(v, s), _mm256_srli_epi32(v, 32-(s)))

//// <summary>
//// MD4 compress block
//// </summary>
//// <param name="state">The md4 state</param>
//// <param name="block">The message to compress</param>
void md4_block_PLAIN_C(uint32_t state[4], uint32_t block[16])
{

	uint32_t a = state[0];
	uint32_t b = state[1];
	uint32_t c = state[2];
	uint32_t d = state[3];


	// Round 1
	a += block[0];	uint32_t t = c ^ d;	t &= b;	a += (d ^ t);	a = rotl32(a, 3);
	d += block[1];	t = b ^ c;	t &= a;	d += (c ^ t);	d = rotl32(d, 7);
	c += block[2];	t = a ^ b;	t &= d;	c += (b ^ t);	c = rotl32(c, 11);
	b += block[3];	t = d ^ a;	t &= c;	b += (a ^ t);	b = rotl32(b, 19);
	a += block[4];	t = c ^ d;	t &= b;	a += (d ^ t);	a = rotl32(a, 3);
	d += block[5];	t = b ^ c;	t &= a;	d += (c ^ t);	d = rotl32(d, 7);
	c += block[6];	t = a ^ b;	t &= d;	c += (b ^ t);	c = rotl32(c, 11);
	b += block[7];	t = d ^ a;	t &= c;	b += (a ^ t);	b = rotl32(b, 19);
	a += block[8];	t = c ^ d;	t &= b;	a += (d ^ t);	a = rotl32(a, 3);
	d += block[9];	t = b ^ c;	t &= a;	d += (c ^ t);	d = rotl32(d, 7);
	c += block[10];	t = a ^ b;	t &= d;	c += (b ^ t);	c = rotl32(c, 11);
	b += block[11];	t = d ^ a;	t &= c;	b += (a ^ t);	b = rotl32(b, 19);
	a += block[12];	t = c ^ d;	t &= b;	a += (d ^ t);	a = rotl32(a, 3);
	d += block[13];	t = b ^ c;	t &= a;	d += (c ^ t);	d = rotl32(d, 7);
	c += block[14];	t = a ^ b;	t &= d;	c += (b ^ t);	c = rotl32(c, 11);
	b += block[15];	t = d ^ a;	t &= c;	b += (a ^ t);	b = rotl32(b, 19);


	// Round 2
	a += block[0];	t = b & c;	uint32_t tt = b ^ c;	a += 0x5a827999;	a += (t ^ (d & tt));	a = rotl32(a, 3);
	d += block[4];	d += 0x5a827999;	d += (t ^ (a & tt));	d = rotl32(d, 5);
	c += block[8];	t = a & d;	tt = a ^ d;	c += 0x5a827999;	c += (t ^ (b & tt));	c = rotl32(c, 9);
	b += block[12];	b += 0x5a827999;	b += (t ^ (c & tt));	b = rotl32(b, 13);
	a += block[1];	t = b & c;	tt = b ^ c;	a += 0x5a827999;	a += (t ^ (d & tt));	a = rotl32(a, 3);
	d += block[5];	d += 0x5a827999;	d += (t ^ (a & tt));	d = rotl32(d, 5);
	c += block[9];	t = a & d;	tt = a ^ d;	c += 0x5a827999;	c += (t ^ (b & tt));	c = rotl32(c, 9);
	b += block[13];	b += 0x5a827999;	b += (t ^ (c & tt));	b = rotl32(b, 13);
	a += block[2];	t = b & c;	tt = b ^ c;	a += 0x5a827999;	a += (t ^ (d & tt));	a = rotl32(a, 3);
	d += block[6];	d += 0x5a827999;	d += (t ^ (a & tt));	d = rotl32(d, 5);
	c += block[10];	t = a & d;	tt = a ^ d;	c += 0x5a827999;	c += (t ^ (b & tt));	c = rotl32(c, 9);
	b += block[14];	b += 0x5a827999;	b += (t ^ (c & tt));	b = rotl32(b, 13);
	a += block[3];	t = b & c;	tt = b ^ c;	a += 0x5a827999;	a += (t ^ (d & tt));	a = rotl32(a, 3);
	d += block[7];	d += 0x5a827999;	d += (t ^ (a & tt));	d = rotl32(d, 5);
	c += block[11];	t = a & d;	tt = a ^ d;	c += 0x5a827999;	c += (t ^ (b & tt));	c = rotl32(c, 9);
	b += block[15];	b += 0x5a827999;	b += (t ^ (c & tt));	b = rotl32(b, 13);


	// Round 3 
	a += block[0];	t = b ^ c;	a += 0x6ed9eba1;	a += (t ^ d);	a = rotl32(a, 3);
	d += block[8];	d += 0x6ed9eba1;	d += (t ^ a);	d = rotl32(d, 9);
	c += block[4];	t = a ^ d;	c += 0x6ed9eba1;	c += (t ^ b);	c = rotl32(c, 11);
	b += block[12];	b += 0x6ed9eba1;	b += (t ^ c);	b = rotl32(b, 15);
	a += block[2];	t = b ^ c;	a += 0x6ed9eba1;	a += (t ^ d);	a = rotl32(a, 3);
	d += block[10];	d += 0x6ed9eba1;	d += (t ^ a);	d = rotl32(d, 9);
	c += block[6];	t = a ^ d;	c += 0x6ed9eba1;	c += (t ^ b);	c = rotl32(c, 11);
	b += block[14];	b += 0x6ed9eba1;	b += (t ^ c);	b = rotl32(b, 15);
	a += block[1];	t = b ^ c;	a += 0x6ed9eba1;	a += (t ^ d);	a = rotl32(a, 3);
	d += block[9];	d += 0x6ed9eba1;	d += (t ^ a);	d = rotl32(d, 9);
	c += block[5];	t = a ^ d;	c += 0x6ed9eba1;	c += (t ^ b);	c = rotl32(c, 11);
	b += block[13];	b += 0x6ed9eba1;	b += (t ^ c);	b = rotl32(b, 15);
	a += block[3];	t = b ^ c;	a += 0x6ed9eba1;	a += (t ^ d);	a = rotl32(a, 3);
	d += block[11];	d += 0x6ed9eba1;	d += (t ^ a);	d = rotl32(d, 9);
	c += block[7];	t = a ^ d;	c += 0x6ed9eba1;	c += (t ^ b);	c = rotl32(c, 11);
	b += block[15];	b += 0x6ed9eba1;	b += (t ^ c);	b = rotl32(b, 15);

	state[0] = (state[0] + a);
	state[1] = (state[1] + b);
	state[2] = (state[2] + c);
	state[3] = (state[3] + d);
}

void md4_block_AVX2_INTRINSICS(__m256i state[12], __m256i block[48])
{

	__m256i a0 = _mm256_load_si256(state + 0);	__m256i a1 = _mm256_load_si256(state + 1);	__m256i a2 = _mm256_load_si256(state + 2);
	__m256i b0 = _mm256_load_si256(state + 3);	__m256i b1 = _mm256_load_si256(state + 4);	__m256i b2 = _mm256_load_si256(state + 5);
	__m256i c0 = _mm256_load_si256(state + 6);	__m256i c1 = _mm256_load_si256(state + 7);	__m256i c2 = _mm256_load_si256(state + 8);
	__m256i d0 = _mm256_load_si256(state + 9);	__m256i d1 = _mm256_load_si256(state + 10);	__m256i d2 = _mm256_load_si256(state + 11);


	// Round 1
	a0 = _mm256_add_epi32(a0, _mm256_load_si256(block + 0));	a1 = _mm256_add_epi32(a1, _mm256_load_si256(block + 1));	a2 = _mm256_add_epi32(a2, _mm256_load_si256(block + 2));	__m256i t0 = _mm256_xor_si256(c0, d0);	__m256i t1 = _mm256_xor_si256(c1, d1);	__m256i t2 = _mm256_xor_si256(c2, d2);	t0 = _mm256_and_si256(t0, b0);	t1 = _mm256_and_si256(t1, b1);	t2 = _mm256_and_si256(t2, b2);	a0 = _mm256_add_epi32(a0, _mm256_xor_si256(d0, t0));	a1 = _mm256_add_epi32(a1, _mm256_xor_si256(d1, t1));	a2 = _mm256_add_epi32(a2, _mm256_xor_si256(d2, t2));	a0 = _mm256_or_si256(_mm256_slli_epi32(a0, 3), _mm256_srli_epi32(a0, 29));	a1 = _mm256_or_si256(_mm256_slli_epi32(a1, 3), _mm256_srli_epi32(a1, 29));	a2 = _mm256_or_si256(_mm256_slli_epi32(a2, 3), _mm256_srli_epi32(a2, 29));
	d0 = _mm256_add_epi32(d0, _mm256_load_si256(block + 3));	d1 = _mm256_add_epi32(d1, _mm256_load_si256(block + 4));	d2 = _mm256_add_epi32(d2, _mm256_load_si256(block + 5));	t0 = _mm256_xor_si256(b0, c0);	t1 = _mm256_xor_si256(b1, c1);	t2 = _mm256_xor_si256(b2, c2);	t0 = _mm256_and_si256(t0, a0);	t1 = _mm256_and_si256(t1, a1);	t2 = _mm256_and_si256(t2, a2);	d0 = _mm256_add_epi32(d0, _mm256_xor_si256(c0, t0));	d1 = _mm256_add_epi32(d1, _mm256_xor_si256(c1, t1));	d2 = _mm256_add_epi32(d2, _mm256_xor_si256(c2, t2));	d0 = _mm256_or_si256(_mm256_slli_epi32(d0, 7), _mm256_srli_epi32(d0, 25));	d1 = _mm256_or_si256(_mm256_slli_epi32(d1, 7), _mm256_srli_epi32(d1, 25));	d2 = _mm256_or_si256(_mm256_slli_epi32(d2, 7), _mm256_srli_epi32(d2, 25));
	c0 = _mm256_add_epi32(c0, _mm256_load_si256(block + 6));	c1 = _mm256_add_epi32(c1, _mm256_load_si256(block + 7));	c2 = _mm256_add_epi32(c2, _mm256_load_si256(block + 8));	t0 = _mm256_xor_si256(a0, b0);	t1 = _mm256_xor_si256(a1, b1);	t2 = _mm256_xor_si256(a2, b2);	t0 = _mm256_and_si256(t0, d0);	t1 = _mm256_and_si256(t1, d1);	t2 = _mm256_and_si256(t2, d2);	c0 = _mm256_add_epi32(c0, _mm256_xor_si256(b0, t0));	c1 = _mm256_add_epi32(c1, _mm256_xor_si256(b1, t1));	c2 = _mm256_add_epi32(c2, _mm256_xor_si256(b2, t2));	c0 = _mm256_or_si256(_mm256_slli_epi32(c0, 11), _mm256_srli_epi32(c0, 21));	c1 = _mm256_or_si256(_mm256_slli_epi32(c1, 11), _mm256_srli_epi32(c1, 21));	c2 = _mm256_or_si256(_mm256_slli_epi32(c2, 11), _mm256_srli_epi32(c2, 21));
	b0 = _mm256_add_epi32(b0, _mm256_load_si256(block + 9));	b1 = _mm256_add_epi32(b1, _mm256_load_si256(block + 10));	b2 = _mm256_add_epi32(b2, _mm256_load_si256(block + 11));	t0 = _mm256_xor_si256(d0, a0);	t1 = _mm256_xor_si256(d1, a1);	t2 = _mm256_xor_si256(d2, a2);	t0 = _mm256_and_si256(t0, c0);	t1 = _mm256_and_si256(t1, c1);	t2 = _mm256_and_si256(t2, c2);	b0 = _mm256_add_epi32(b0, _mm256_xor_si256(a0, t0));	b1 = _mm256_add_epi32(b1, _mm256_xor_si256(a1, t1));	b2 = _mm256_add_epi32(b2, _mm256_xor_si256(a2, t2));	b0 = _mm256_or_si256(_mm256_slli_epi32(b0, 19), _mm256_srli_epi32(b0, 13));	b1 = _mm256_or_si256(_mm256_slli_epi32(b1, 19), _mm256_srli_epi32(b1, 13));	b2 = _mm256_or_si256(_mm256_slli_epi32(b2, 19), _mm256_srli_epi32(b2, 13));
	a0 = _mm256_add_epi32(a0, _mm256_load_si256(block + 12));	a1 = _mm256_add_epi32(a1, _mm256_load_si256(block + 13));	a2 = _mm256_add_epi32(a2, _mm256_load_si256(block + 14));	t0 = _mm256_xor_si256(c0, d0);	t1 = _mm256_xor_si256(c1, d1);	t2 = _mm256_xor_si256(c2, d2);	t0 = _mm256_and_si256(t0, b0);	t1 = _mm256_and_si256(t1, b1);	t2 = _mm256_and_si256(t2, b2);	a0 = _mm256_add_epi32(a0, _mm256_xor_si256(d0, t0));	a1 = _mm256_add_epi32(a1, _mm256_xor_si256(d1, t1));	a2 = _mm256_add_epi32(a2, _mm256_xor_si256(d2, t2));	a0 = _mm256_or_si256(_mm256_slli_epi32(a0, 3), _mm256_srli_epi32(a0, 29));	a1 = _mm256_or_si256(_mm256_slli_epi32(a1, 3), _mm256_srli_epi32(a1, 29));	a2 = _mm256_or_si256(_mm256_slli_epi32(a2, 3), _mm256_srli_epi32(a2, 29));
	d0 = _mm256_add_epi32(d0, _mm256_load_si256(block + 15));	d1 = _mm256_add_epi32(d1, _mm256_load_si256(block + 16));	d2 = _mm256_add_epi32(d2, _mm256_load_si256(block + 17));	t0 = _mm256_xor_si256(b0, c0);	t1 = _mm256_xor_si256(b1, c1);	t2 = _mm256_xor_si256(b2, c2);	t0 = _mm256_and_si256(t0, a0);	t1 = _mm256_and_si256(t1, a1);	t2 = _mm256_and_si256(t2, a2);	d0 = _mm256_add_epi32(d0, _mm256_xor_si256(c0, t0));	d1 = _mm256_add_epi32(d1, _mm256_xor_si256(c1, t1));	d2 = _mm256_add_epi32(d2, _mm256_xor_si256(c2, t2));	d0 = _mm256_or_si256(_mm256_slli_epi32(d0, 7), _mm256_srli_epi32(d0, 25));	d1 = _mm256_or_si256(_mm256_slli_epi32(d1, 7), _mm256_srli_epi32(d1, 25));	d2 = _mm256_or_si256(_mm256_slli_epi32(d2, 7), _mm256_srli_epi32(d2, 25));
	c0 = _mm256_add_epi32(c0, _mm256_load_si256(block + 18));	c1 = _mm256_add_epi32(c1, _mm256_load_si256(block + 19));	c2 = _mm256_add_epi32(c2, _mm256_load_si256(block + 20));	t0 = _mm256_xor_si256(a0, b0);	t1 = _mm256_xor_si256(a1, b1);	t2 = _mm256_xor_si256(a2, b2);	t0 = _mm256_and_si256(t0, d0);	t1 = _mm256_and_si256(t1, d1);	t2 = _mm256_and_si256(t2, d2);	c0 = _mm256_add_epi32(c0, _mm256_xor_si256(b0, t0));	c1 = _mm256_add_epi32(c1, _mm256_xor_si256(b1, t1));	c2 = _mm256_add_epi32(c2, _mm256_xor_si256(b2, t2));	c0 = _mm256_or_si256(_mm256_slli_epi32(c0, 11), _mm256_srli_epi32(c0, 21));	c1 = _mm256_or_si256(_mm256_slli_epi32(c1, 11), _mm256_srli_epi32(c1, 21));	c2 = _mm256_or_si256(_mm256_slli_epi32(c2, 11), _mm256_srli_epi32(c2, 21));
	b0 = _mm256_add_epi32(b0, _mm256_load_si256(block + 21));	b1 = _mm256_add_epi32(b1, _mm256_load_si256(block + 22));	b2 = _mm256_add_epi32(b2, _mm256_load_si256(block + 23));	t0 = _mm256_xor_si256(d0, a0);	t1 = _mm256_xor_si256(d1, a1);	t2 = _mm256_xor_si256(d2, a2);	t0 = _mm256_and_si256(t0, c0);	t1 = _mm256_and_si256(t1, c1);	t2 = _mm256_and_si256(t2, c2);	b0 = _mm256_add_epi32(b0, _mm256_xor_si256(a0, t0));	b1 = _mm256_add_epi32(b1, _mm256_xor_si256(a1, t1));	b2 = _mm256_add_epi32(b2, _mm256_xor_si256(a2, t2));	b0 = _mm256_or_si256(_mm256_slli_epi32(b0, 19), _mm256_srli_epi32(b0, 13));	b1 = _mm256_or_si256(_mm256_slli_epi32(b1, 19), _mm256_srli_epi32(b1, 13));	b2 = _mm256_or_si256(_mm256_slli_epi32(b2, 19), _mm256_srli_epi32(b2, 13));
	a0 = _mm256_add_epi32(a0, _mm256_load_si256(block + 24));	a1 = _mm256_add_epi32(a1, _mm256_load_si256(block + 25));	a2 = _mm256_add_epi32(a2, _mm256_load_si256(block + 26));	t0 = _mm256_xor_si256(c0, d0);	t1 = _mm256_xor_si256(c1, d1);	t2 = _mm256_xor_si256(c2, d2);	t0 = _mm256_and_si256(t0, b0);	t1 = _mm256_and_si256(t1, b1);	t2 = _mm256_and_si256(t2, b2);	a0 = _mm256_add_epi32(a0, _mm256_xor_si256(d0, t0));	a1 = _mm256_add_epi32(a1, _mm256_xor_si256(d1, t1));	a2 = _mm256_add_epi32(a2, _mm256_xor_si256(d2, t2));	a0 = _mm256_or_si256(_mm256_slli_epi32(a0, 3), _mm256_srli_epi32(a0, 29));	a1 = _mm256_or_si256(_mm256_slli_epi32(a1, 3), _mm256_srli_epi32(a1, 29));	a2 = _mm256_or_si256(_mm256_slli_epi32(a2, 3), _mm256_srli_epi32(a2, 29));
	d0 = _mm256_add_epi32(d0, _mm256_load_si256(block + 27));	d1 = _mm256_add_epi32(d1, _mm256_load_si256(block + 28));	d2 = _mm256_add_epi32(d2, _mm256_load_si256(block + 29));	t0 = _mm256_xor_si256(b0, c0);	t1 = _mm256_xor_si256(b1, c1);	t2 = _mm256_xor_si256(b2, c2);	t0 = _mm256_and_si256(t0, a0);	t1 = _mm256_and_si256(t1, a1);	t2 = _mm256_and_si256(t2, a2);	d0 = _mm256_add_epi32(d0, _mm256_xor_si256(c0, t0));	d1 = _mm256_add_epi32(d1, _mm256_xor_si256(c1, t1));	d2 = _mm256_add_epi32(d2, _mm256_xor_si256(c2, t2));	d0 = _mm256_or_si256(_mm256_slli_epi32(d0, 7), _mm256_srli_epi32(d0, 25));	d1 = _mm256_or_si256(_mm256_slli_epi32(d1, 7), _mm256_srli_epi32(d1, 25));	d2 = _mm256_or_si256(_mm256_slli_epi32(d2, 7), _mm256_srli_epi32(d2, 25));
	c0 = _mm256_add_epi32(c0, _mm256_load_si256(block + 30));	c1 = _mm256_add_epi32(c1, _mm256_load_si256(block + 31));	c2 = _mm256_add_epi32(c2, _mm256_load_si256(block + 32));	t0 = _mm256_xor_si256(a0, b0);	t1 = _mm256_xor_si256(a1, b1);	t2 = _mm256_xor_si256(a2, b2);	t0 = _mm256_and_si256(t0, d0);	t1 = _mm256_and_si256(t1, d1);	t2 = _mm256_and_si256(t2, d2);	c0 = _mm256_add_epi32(c0, _mm256_xor_si256(b0, t0));	c1 = _mm256_add_epi32(c1, _mm256_xor_si256(b1, t1));	c2 = _mm256_add_epi32(c2, _mm256_xor_si256(b2, t2));	c0 = _mm256_or_si256(_mm256_slli_epi32(c0, 11), _mm256_srli_epi32(c0, 21));	c1 = _mm256_or_si256(_mm256_slli_epi32(c1, 11), _mm256_srli_epi32(c1, 21));	c2 = _mm256_or_si256(_mm256_slli_epi32(c2, 11), _mm256_srli_epi32(c2, 21));
	b0 = _mm256_add_epi32(b0, _mm256_load_si256(block + 33));	b1 = _mm256_add_epi32(b1, _mm256_load_si256(block + 34));	b2 = _mm256_add_epi32(b2, _mm256_load_si256(block + 35));	t0 = _mm256_xor_si256(d0, a0);	t1 = _mm256_xor_si256(d1, a1);	t2 = _mm256_xor_si256(d2, a2);	t0 = _mm256_and_si256(t0, c0);	t1 = _mm256_and_si256(t1, c1);	t2 = _mm256_and_si256(t2, c2);	b0 = _mm256_add_epi32(b0, _mm256_xor_si256(a0, t0));	b1 = _mm256_add_epi32(b1, _mm256_xor_si256(a1, t1));	b2 = _mm256_add_epi32(b2, _mm256_xor_si256(a2, t2));	b0 = _mm256_or_si256(_mm256_slli_epi32(b0, 19), _mm256_srli_epi32(b0, 13));	b1 = _mm256_or_si256(_mm256_slli_epi32(b1, 19), _mm256_srli_epi32(b1, 13));	b2 = _mm256_or_si256(_mm256_slli_epi32(b2, 19), _mm256_srli_epi32(b2, 13));
	a0 = _mm256_add_epi32(a0, _mm256_load_si256(block + 36));	a1 = _mm256_add_epi32(a1, _mm256_load_si256(block + 37));	a2 = _mm256_add_epi32(a2, _mm256_load_si256(block + 38));	t0 = _mm256_xor_si256(c0, d0);	t1 = _mm256_xor_si256(c1, d1);	t2 = _mm256_xor_si256(c2, d2);	t0 = _mm256_and_si256(t0, b0);	t1 = _mm256_and_si256(t1, b1);	t2 = _mm256_and_si256(t2, b2);	a0 = _mm256_add_epi32(a0, _mm256_xor_si256(d0, t0));	a1 = _mm256_add_epi32(a1, _mm256_xor_si256(d1, t1));	a2 = _mm256_add_epi32(a2, _mm256_xor_si256(d2, t2));	a0 = _mm256_or_si256(_mm256_slli_epi32(a0, 3), _mm256_srli_epi32(a0, 29));	a1 = _mm256_or_si256(_mm256_slli_epi32(a1, 3), _mm256_srli_epi32(a1, 29));	a2 = _mm256_or_si256(_mm256_slli_epi32(a2, 3), _mm256_srli_epi32(a2, 29));
	d0 = _mm256_add_epi32(d0, _mm256_load_si256(block + 39));	d1 = _mm256_add_epi32(d1, _mm256_load_si256(block + 40));	d2 = _mm256_add_epi32(d2, _mm256_load_si256(block + 41));	t0 = _mm256_xor_si256(b0, c0);	t1 = _mm256_xor_si256(b1, c1);	t2 = _mm256_xor_si256(b2, c2);	t0 = _mm256_and_si256(t0, a0);	t1 = _mm256_and_si256(t1, a1);	t2 = _mm256_and_si256(t2, a2);	d0 = _mm256_add_epi32(d0, _mm256_xor_si256(c0, t0));	d1 = _mm256_add_epi32(d1, _mm256_xor_si256(c1, t1));	d2 = _mm256_add_epi32(d2, _mm256_xor_si256(c2, t2));	d0 = _mm256_or_si256(_mm256_slli_epi32(d0, 7), _mm256_srli_epi32(d0, 25));	d1 = _mm256_or_si256(_mm256_slli_epi32(d1, 7), _mm256_srli_epi32(d1, 25));	d2 = _mm256_or_si256(_mm256_slli_epi32(d2, 7), _mm256_srli_epi32(d2, 25));
	c0 = _mm256_add_epi32(c0, _mm256_load_si256(block + 42));	c1 = _mm256_add_epi32(c1, _mm256_load_si256(block + 43));	c2 = _mm256_add_epi32(c2, _mm256_load_si256(block + 44));	t0 = _mm256_xor_si256(a0, b0);	t1 = _mm256_xor_si256(a1, b1);	t2 = _mm256_xor_si256(a2, b2);	t0 = _mm256_and_si256(t0, d0);	t1 = _mm256_and_si256(t1, d1);	t2 = _mm256_and_si256(t2, d2);	c0 = _mm256_add_epi32(c0, _mm256_xor_si256(b0, t0));	c1 = _mm256_add_epi32(c1, _mm256_xor_si256(b1, t1));	c2 = _mm256_add_epi32(c2, _mm256_xor_si256(b2, t2));	c0 = _mm256_or_si256(_mm256_slli_epi32(c0, 11), _mm256_srli_epi32(c0, 21));	c1 = _mm256_or_si256(_mm256_slli_epi32(c1, 11), _mm256_srli_epi32(c1, 21));	c2 = _mm256_or_si256(_mm256_slli_epi32(c2, 11), _mm256_srli_epi32(c2, 21));
	b0 = _mm256_add_epi32(b0, _mm256_load_si256(block + 45));	b1 = _mm256_add_epi32(b1, _mm256_load_si256(block + 46));	b2 = _mm256_add_epi32(b2, _mm256_load_si256(block + 47));	t0 = _mm256_xor_si256(d0, a0);	t1 = _mm256_xor_si256(d1, a1);	t2 = _mm256_xor_si256(d2, a2);	t0 = _mm256_and_si256(t0, c0);	t1 = _mm256_and_si256(t1, c1);	t2 = _mm256_and_si256(t2, c2);	b0 = _mm256_add_epi32(b0, _mm256_xor_si256(a0, t0));	b1 = _mm256_add_epi32(b1, _mm256_xor_si256(a1, t1));	b2 = _mm256_add_epi32(b2, _mm256_xor_si256(a2, t2));	b0 = _mm256_or_si256(_mm256_slli_epi32(b0, 19), _mm256_srli_epi32(b0, 13));	b1 = _mm256_or_si256(_mm256_slli_epi32(b1, 19), _mm256_srli_epi32(b1, 13));	b2 = _mm256_or_si256(_mm256_slli_epi32(b2, 19), _mm256_srli_epi32(b2, 13));


	// Round 2
	a0 = _mm256_add_epi32(a0, _mm256_load_si256(block + 0));	a1 = _mm256_add_epi32(a1, _mm256_load_si256(block + 1));	a2 = _mm256_add_epi32(a2, _mm256_load_si256(block + 2));	t0 = _mm256_and_si256(b0, c0);	t1 = _mm256_and_si256(b1, c1);	t2 = _mm256_and_si256(b2, c2);	__m256i tt0 = _mm256_xor_si256(b0, c0);	__m256i tt1 = _mm256_xor_si256(b1, c1);	__m256i tt2 = _mm256_xor_si256(b2, c2);	a0 = _mm256_add_epi32(a0, _mm256_set1_epi32(0x5a827999));	a1 = _mm256_add_epi32(a1, _mm256_set1_epi32(0x5a827999));	a2 = _mm256_add_epi32(a2, _mm256_set1_epi32(0x5a827999));	a0 = _mm256_add_epi32(a0, _mm256_xor_si256(t0, _mm256_and_si256(d0, tt0)));	a1 = _mm256_add_epi32(a1, _mm256_xor_si256(t1, _mm256_and_si256(d1, tt1)));	a2 = _mm256_add_epi32(a2, _mm256_xor_si256(t2, _mm256_and_si256(d2, tt2)));	a0 = _mm256_or_si256(_mm256_slli_epi32(a0, 3), _mm256_srli_epi32(a0, 29));	a1 = _mm256_or_si256(_mm256_slli_epi32(a1, 3), _mm256_srli_epi32(a1, 29));	a2 = _mm256_or_si256(_mm256_slli_epi32(a2, 3), _mm256_srli_epi32(a2, 29));
	d0 = _mm256_add_epi32(d0, _mm256_load_si256(block + 12));	d1 = _mm256_add_epi32(d1, _mm256_load_si256(block + 13));	d2 = _mm256_add_epi32(d2, _mm256_load_si256(block + 14));	d0 = _mm256_add_epi32(d0, _mm256_set1_epi32(0x5a827999));	d1 = _mm256_add_epi32(d1, _mm256_set1_epi32(0x5a827999));	d2 = _mm256_add_epi32(d2, _mm256_set1_epi32(0x5a827999));	d0 = _mm256_add_epi32(d0, _mm256_xor_si256(t0, _mm256_and_si256(a0, tt0)));	d1 = _mm256_add_epi32(d1, _mm256_xor_si256(t1, _mm256_and_si256(a1, tt1)));	d2 = _mm256_add_epi32(d2, _mm256_xor_si256(t2, _mm256_and_si256(a2, tt2)));	d0 = _mm256_or_si256(_mm256_slli_epi32(d0, 5), _mm256_srli_epi32(d0, 27));	d1 = _mm256_or_si256(_mm256_slli_epi32(d1, 5), _mm256_srli_epi32(d1, 27));	d2 = _mm256_or_si256(_mm256_slli_epi32(d2, 5), _mm256_srli_epi32(d2, 27));
	c0 = _mm256_add_epi32(c0, _mm256_load_si256(block + 24));	c1 = _mm256_add_epi32(c1, _mm256_load_si256(block + 25));	c2 = _mm256_add_epi32(c2, _mm256_load_si256(block + 26));	t0 = _mm256_and_si256(a0, d0);	t1 = _mm256_and_si256(a1, d1);	t2 = _mm256_and_si256(a2, d2);	tt0 = _mm256_xor_si256(a0, d0);	tt1 = _mm256_xor_si256(a1, d1);	tt2 = _mm256_xor_si256(a2, d2);	c0 = _mm256_add_epi32(c0, _mm256_set1_epi32(0x5a827999));	c1 = _mm256_add_epi32(c1, _mm256_set1_epi32(0x5a827999));	c2 = _mm256_add_epi32(c2, _mm256_set1_epi32(0x5a827999));	c0 = _mm256_add_epi32(c0, _mm256_xor_si256(t0, _mm256_and_si256(b0, tt0)));	c1 = _mm256_add_epi32(c1, _mm256_xor_si256(t1, _mm256_and_si256(b1, tt1)));	c2 = _mm256_add_epi32(c2, _mm256_xor_si256(t2, _mm256_and_si256(b2, tt2)));	c0 = _mm256_or_si256(_mm256_slli_epi32(c0, 9), _mm256_srli_epi32(c0, 23));	c1 = _mm256_or_si256(_mm256_slli_epi32(c1, 9), _mm256_srli_epi32(c1, 23));	c2 = _mm256_or_si256(_mm256_slli_epi32(c2, 9), _mm256_srli_epi32(c2, 23));
	b0 = _mm256_add_epi32(b0, _mm256_load_si256(block + 36));	b1 = _mm256_add_epi32(b1, _mm256_load_si256(block + 37));	b2 = _mm256_add_epi32(b2, _mm256_load_si256(block + 38));	b0 = _mm256_add_epi32(b0, _mm256_set1_epi32(0x5a827999));	b1 = _mm256_add_epi32(b1, _mm256_set1_epi32(0x5a827999));	b2 = _mm256_add_epi32(b2, _mm256_set1_epi32(0x5a827999));	b0 = _mm256_add_epi32(b0, _mm256_xor_si256(t0, _mm256_and_si256(c0, tt0)));	b1 = _mm256_add_epi32(b1, _mm256_xor_si256(t1, _mm256_and_si256(c1, tt1)));	b2 = _mm256_add_epi32(b2, _mm256_xor_si256(t2, _mm256_and_si256(c2, tt2)));	b0 = _mm256_or_si256(_mm256_slli_epi32(b0, 13), _mm256_srli_epi32(b0, 19));	b1 = _mm256_or_si256(_mm256_slli_epi32(b1, 13), _mm256_srli_epi32(b1, 19));	b2 = _mm256_or_si256(_mm256_slli_epi32(b2, 13), _mm256_srli_epi32(b2, 19));
	a0 = _mm256_add_epi32(a0, _mm256_load_si256(block + 3));	a1 = _mm256_add_epi32(a1, _mm256_load_si256(block + 4));	a2 = _mm256_add_epi32(a2, _mm256_load_si256(block + 5));	t0 = _mm256_and_si256(b0, c0);	t1 = _mm256_and_si256(b1, c1);	t2 = _mm256_and_si256(b2, c2);	tt0 = _mm256_xor_si256(b0, c0);	tt1 = _mm256_xor_si256(b1, c1);	tt2 = _mm256_xor_si256(b2, c2);	a0 = _mm256_add_epi32(a0, _mm256_set1_epi32(0x5a827999));	a1 = _mm256_add_epi32(a1, _mm256_set1_epi32(0x5a827999));	a2 = _mm256_add_epi32(a2, _mm256_set1_epi32(0x5a827999));	a0 = _mm256_add_epi32(a0, _mm256_xor_si256(t0, _mm256_and_si256(d0, tt0)));	a1 = _mm256_add_epi32(a1, _mm256_xor_si256(t1, _mm256_and_si256(d1, tt1)));	a2 = _mm256_add_epi32(a2, _mm256_xor_si256(t2, _mm256_and_si256(d2, tt2)));	a0 = _mm256_or_si256(_mm256_slli_epi32(a0, 3), _mm256_srli_epi32(a0, 29));	a1 = _mm256_or_si256(_mm256_slli_epi32(a1, 3), _mm256_srli_epi32(a1, 29));	a2 = _mm256_or_si256(_mm256_slli_epi32(a2, 3), _mm256_srli_epi32(a2, 29));
	d0 = _mm256_add_epi32(d0, _mm256_load_si256(block + 15));	d1 = _mm256_add_epi32(d1, _mm256_load_si256(block + 16));	d2 = _mm256_add_epi32(d2, _mm256_load_si256(block + 17));	d0 = _mm256_add_epi32(d0, _mm256_set1_epi32(0x5a827999));	d1 = _mm256_add_epi32(d1, _mm256_set1_epi32(0x5a827999));	d2 = _mm256_add_epi32(d2, _mm256_set1_epi32(0x5a827999));	d0 = _mm256_add_epi32(d0, _mm256_xor_si256(t0, _mm256_and_si256(a0, tt0)));	d1 = _mm256_add_epi32(d1, _mm256_xor_si256(t1, _mm256_and_si256(a1, tt1)));	d2 = _mm256_add_epi32(d2, _mm256_xor_si256(t2, _mm256_and_si256(a2, tt2)));	d0 = _mm256_or_si256(_mm256_slli_epi32(d0, 5), _mm256_srli_epi32(d0, 27));	d1 = _mm256_or_si256(_mm256_slli_epi32(d1, 5), _mm256_srli_epi32(d1, 27));	d2 = _mm256_or_si256(_mm256_slli_epi32(d2, 5), _mm256_srli_epi32(d2, 27));
	c0 = _mm256_add_epi32(c0, _mm256_load_si256(block + 27));	c1 = _mm256_add_epi32(c1, _mm256_load_si256(block + 28));	c2 = _mm256_add_epi32(c2, _mm256_load_si256(block + 29));	t0 = _mm256_and_si256(a0, d0);	t1 = _mm256_and_si256(a1, d1);	t2 = _mm256_and_si256(a2, d2);	tt0 = _mm256_xor_si256(a0, d0);	tt1 = _mm256_xor_si256(a1, d1);	tt2 = _mm256_xor_si256(a2, d2);	c0 = _mm256_add_epi32(c0, _mm256_set1_epi32(0x5a827999));	c1 = _mm256_add_epi32(c1, _mm256_set1_epi32(0x5a827999));	c2 = _mm256_add_epi32(c2, _mm256_set1_epi32(0x5a827999));	c0 = _mm256_add_epi32(c0, _mm256_xor_si256(t0, _mm256_and_si256(b0, tt0)));	c1 = _mm256_add_epi32(c1, _mm256_xor_si256(t1, _mm256_and_si256(b1, tt1)));	c2 = _mm256_add_epi32(c2, _mm256_xor_si256(t2, _mm256_and_si256(b2, tt2)));	c0 = _mm256_or_si256(_mm256_slli_epi32(c0, 9), _mm256_srli_epi32(c0, 23));	c1 = _mm256_or_si256(_mm256_slli_epi32(c1, 9), _mm256_srli_epi32(c1, 23));	c2 = _mm256_or_si256(_mm256_slli_epi32(c2, 9), _mm256_srli_epi32(c2, 23));
	b0 = _mm256_add_epi32(b0, _mm256_load_si256(block + 39));	b1 = _mm256_add_epi32(b1, _mm256_load_si256(block + 40));	b2 = _mm256_add_epi32(b2, _mm256_load_si256(block + 41));	b0 = _mm256_add_epi32(b0, _mm256_set1_epi32(0x5a827999));	b1 = _mm256_add_epi32(b1, _mm256_set1_epi32(0x5a827999));	b2 = _mm256_add_epi32(b2, _mm256_set1_epi32(0x5a827999));	b0 = _mm256_add_epi32(b0, _mm256_xor_si256(t0, _mm256_and_si256(c0, tt0)));	b1 = _mm256_add_epi32(b1, _mm256_xor_si256(t1, _mm256_and_si256(c1, tt1)));	b2 = _mm256_add_epi32(b2, _mm256_xor_si256(t2, _mm256_and_si256(c2, tt2)));	b0 = _mm256_or_si256(_mm256_slli_epi32(b0, 13), _mm256_srli_epi32(b0, 19));	b1 = _mm256_or_si256(_mm256_slli_epi32(b1, 13), _mm256_srli_epi32(b1, 19));	b2 = _mm256_or_si256(_mm256_slli_epi32(b2, 13), _mm256_srli_epi32(b2, 19));
	a0 = _mm256_add_epi32(a0, _mm256_load_si256(block + 6));	a1 = _mm256_add_epi32(a1, _mm256_load_si256(block + 7));	a2 = _mm256_add_epi32(a2, _mm256_load_si256(block + 8));	t0 = _mm256_and_si256(b0, c0);	t1 = _mm256_and_si256(b1, c1);	t2 = _mm256_and_si256(b2, c2);	tt0 = _mm256_xor_si256(b0, c0);	tt1 = _mm256_xor_si256(b1, c1);	tt2 = _mm256_xor_si256(b2, c2);	a0 = _mm256_add_epi32(a0, _mm256_set1_epi32(0x5a827999));	a1 = _mm256_add_epi32(a1, _mm256_set1_epi32(0x5a827999));	a2 = _mm256_add_epi32(a2, _mm256_set1_epi32(0x5a827999));	a0 = _mm256_add_epi32(a0, _mm256_xor_si256(t0, _mm256_and_si256(d0, tt0)));	a1 = _mm256_add_epi32(a1, _mm256_xor_si256(t1, _mm256_and_si256(d1, tt1)));	a2 = _mm256_add_epi32(a2, _mm256_xor_si256(t2, _mm256_and_si256(d2, tt2)));	a0 = _mm256_or_si256(_mm256_slli_epi32(a0, 3), _mm256_srli_epi32(a0, 29));	a1 = _mm256_or_si256(_mm256_slli_epi32(a1, 3), _mm256_srli_epi32(a1, 29));	a2 = _mm256_or_si256(_mm256_slli_epi32(a2, 3), _mm256_srli_epi32(a2, 29));
	d0 = _mm256_add_epi32(d0, _mm256_load_si256(block + 18));	d1 = _mm256_add_epi32(d1, _mm256_load_si256(block + 19));	d2 = _mm256_add_epi32(d2, _mm256_load_si256(block + 20));	d0 = _mm256_add_epi32(d0, _mm256_set1_epi32(0x5a827999));	d1 = _mm256_add_epi32(d1, _mm256_set1_epi32(0x5a827999));	d2 = _mm256_add_epi32(d2, _mm256_set1_epi32(0x5a827999));	d0 = _mm256_add_epi32(d0, _mm256_xor_si256(t0, _mm256_and_si256(a0, tt0)));	d1 = _mm256_add_epi32(d1, _mm256_xor_si256(t1, _mm256_and_si256(a1, tt1)));	d2 = _mm256_add_epi32(d2, _mm256_xor_si256(t2, _mm256_and_si256(a2, tt2)));	d0 = _mm256_or_si256(_mm256_slli_epi32(d0, 5), _mm256_srli_epi32(d0, 27));	d1 = _mm256_or_si256(_mm256_slli_epi32(d1, 5), _mm256_srli_epi32(d1, 27));	d2 = _mm256_or_si256(_mm256_slli_epi32(d2, 5), _mm256_srli_epi32(d2, 27));
	c0 = _mm256_add_epi32(c0, _mm256_load_si256(block + 30));	c1 = _mm256_add_epi32(c1, _mm256_load_si256(block + 31));	c2 = _mm256_add_epi32(c2, _mm256_load_si256(block + 32));	t0 = _mm256_and_si256(a0, d0);	t1 = _mm256_and_si256(a1, d1);	t2 = _mm256_and_si256(a2, d2);	tt0 = _mm256_xor_si256(a0, d0);	tt1 = _mm256_xor_si256(a1, d1);	tt2 = _mm256_xor_si256(a2, d2);	c0 = _mm256_add_epi32(c0, _mm256_set1_epi32(0x5a827999));	c1 = _mm256_add_epi32(c1, _mm256_set1_epi32(0x5a827999));	c2 = _mm256_add_epi32(c2, _mm256_set1_epi32(0x5a827999));	c0 = _mm256_add_epi32(c0, _mm256_xor_si256(t0, _mm256_and_si256(b0, tt0)));	c1 = _mm256_add_epi32(c1, _mm256_xor_si256(t1, _mm256_and_si256(b1, tt1)));	c2 = _mm256_add_epi32(c2, _mm256_xor_si256(t2, _mm256_and_si256(b2, tt2)));	c0 = _mm256_or_si256(_mm256_slli_epi32(c0, 9), _mm256_srli_epi32(c0, 23));	c1 = _mm256_or_si256(_mm256_slli_epi32(c1, 9), _mm256_srli_epi32(c1, 23));	c2 = _mm256_or_si256(_mm256_slli_epi32(c2, 9), _mm256_srli_epi32(c2, 23));
	b0 = _mm256_add_epi32(b0, _mm256_load_si256(block + 42));	b1 = _mm256_add_epi32(b1, _mm256_load_si256(block + 43));	b2 = _mm256_add_epi32(b2, _mm256_load_si256(block + 44));	b0 = _mm256_add_epi32(b0, _mm256_set1_epi32(0x5a827999));	b1 = _mm256_add_epi32(b1, _mm256_set1_epi32(0x5a827999));	b2 = _mm256_add_epi32(b2, _mm256_set1_epi32(0x5a827999));	b0 = _mm256_add_epi32(b0, _mm256_xor_si256(t0, _mm256_and_si256(c0, tt0)));	b1 = _mm256_add_epi32(b1, _mm256_xor_si256(t1, _mm256_and_si256(c1, tt1)));	b2 = _mm256_add_epi32(b2, _mm256_xor_si256(t2, _mm256_and_si256(c2, tt2)));	b0 = _mm256_or_si256(_mm256_slli_epi32(b0, 13), _mm256_srli_epi32(b0, 19));	b1 = _mm256_or_si256(_mm256_slli_epi32(b1, 13), _mm256_srli_epi32(b1, 19));	b2 = _mm256_or_si256(_mm256_slli_epi32(b2, 13), _mm256_srli_epi32(b2, 19));
	a0 = _mm256_add_epi32(a0, _mm256_load_si256(block + 9));	a1 = _mm256_add_epi32(a1, _mm256_load_si256(block + 10));	a2 = _mm256_add_epi32(a2, _mm256_load_si256(block + 11));	t0 = _mm256_and_si256(b0, c0);	t1 = _mm256_and_si256(b1, c1);	t2 = _mm256_and_si256(b2, c2);	tt0 = _mm256_xor_si256(b0, c0);	tt1 = _mm256_xor_si256(b1, c1);	tt2 = _mm256_xor_si256(b2, c2);	a0 = _mm256_add_epi32(a0, _mm256_set1_epi32(0x5a827999));	a1 = _mm256_add_epi32(a1, _mm256_set1_epi32(0x5a827999));	a2 = _mm256_add_epi32(a2, _mm256_set1_epi32(0x5a827999));	a0 = _mm256_add_epi32(a0, _mm256_xor_si256(t0, _mm256_and_si256(d0, tt0)));	a1 = _mm256_add_epi32(a1, _mm256_xor_si256(t1, _mm256_and_si256(d1, tt1)));	a2 = _mm256_add_epi32(a2, _mm256_xor_si256(t2, _mm256_and_si256(d2, tt2)));	a0 = _mm256_or_si256(_mm256_slli_epi32(a0, 3), _mm256_srli_epi32(a0, 29));	a1 = _mm256_or_si256(_mm256_slli_epi32(a1, 3), _mm256_srli_epi32(a1, 29));	a2 = _mm256_or_si256(_mm256_slli_epi32(a2, 3), _mm256_srli_epi32(a2, 29));
	d0 = _mm256_add_epi32(d0, _mm256_load_si256(block + 21));	d1 = _mm256_add_epi32(d1, _mm256_load_si256(block + 22));	d2 = _mm256_add_epi32(d2, _mm256_load_si256(block + 23));	d0 = _mm256_add_epi32(d0, _mm256_set1_epi32(0x5a827999));	d1 = _mm256_add_epi32(d1, _mm256_set1_epi32(0x5a827999));	d2 = _mm256_add_epi32(d2, _mm256_set1_epi32(0x5a827999));	d0 = _mm256_add_epi32(d0, _mm256_xor_si256(t0, _mm256_and_si256(a0, tt0)));	d1 = _mm256_add_epi32(d1, _mm256_xor_si256(t1, _mm256_and_si256(a1, tt1)));	d2 = _mm256_add_epi32(d2, _mm256_xor_si256(t2, _mm256_and_si256(a2, tt2)));	d0 = _mm256_or_si256(_mm256_slli_epi32(d0, 5), _mm256_srli_epi32(d0, 27));	d1 = _mm256_or_si256(_mm256_slli_epi32(d1, 5), _mm256_srli_epi32(d1, 27));	d2 = _mm256_or_si256(_mm256_slli_epi32(d2, 5), _mm256_srli_epi32(d2, 27));
	c0 = _mm256_add_epi32(c0, _mm256_load_si256(block + 33));	c1 = _mm256_add_epi32(c1, _mm256_load_si256(block + 34));	c2 = _mm256_add_epi32(c2, _mm256_load_si256(block + 35));	t0 = _mm256_and_si256(a0, d0);	t1 = _mm256_and_si256(a1, d1);	t2 = _mm256_and_si256(a2, d2);	tt0 = _mm256_xor_si256(a0, d0);	tt1 = _mm256_xor_si256(a1, d1);	tt2 = _mm256_xor_si256(a2, d2);	c0 = _mm256_add_epi32(c0, _mm256_set1_epi32(0x5a827999));	c1 = _mm256_add_epi32(c1, _mm256_set1_epi32(0x5a827999));	c2 = _mm256_add_epi32(c2, _mm256_set1_epi32(0x5a827999));	c0 = _mm256_add_epi32(c0, _mm256_xor_si256(t0, _mm256_and_si256(b0, tt0)));	c1 = _mm256_add_epi32(c1, _mm256_xor_si256(t1, _mm256_and_si256(b1, tt1)));	c2 = _mm256_add_epi32(c2, _mm256_xor_si256(t2, _mm256_and_si256(b2, tt2)));	c0 = _mm256_or_si256(_mm256_slli_epi32(c0, 9), _mm256_srli_epi32(c0, 23));	c1 = _mm256_or_si256(_mm256_slli_epi32(c1, 9), _mm256_srli_epi32(c1, 23));	c2 = _mm256_or_si256(_mm256_slli_epi32(c2, 9), _mm256_srli_epi32(c2, 23));
	b0 = _mm256_add_epi32(b0, _mm256_load_si256(block + 45));	b1 = _mm256_add_epi32(b1, _mm256_load_si256(block + 46));	b2 = _mm256_add_epi32(b2, _mm256_load_si256(block + 47));	b0 = _mm256_add_epi32(b0, _mm256_set1_epi32(0x5a827999));	b1 = _mm256_add_epi32(b1, _mm256_set1_epi32(0x5a827999));	b2 = _mm256_add_epi32(b2, _mm256_set1_epi32(0x5a827999));	b0 = _mm256_add_epi32(b0, _mm256_xor_si256(t0, _mm256_and_si256(c0, tt0)));	b1 = _mm256_add_epi32(b1, _mm256_xor_si256(t1, _mm256_and_si256(c1, tt1)));	b2 = _mm256_add_epi32(b2, _mm256_xor_si256(t2, _mm256_and_si256(c2, tt2)));	b0 = _mm256_or_si256(_mm256_slli_epi32(b0, 13), _mm256_srli_epi32(b0, 19));	b1 = _mm256_or_si256(_mm256_slli_epi32(b1, 13), _mm256_srli_epi32(b1, 19));	b2 = _mm256_or_si256(_mm256_slli_epi32(b2, 13), _mm256_srli_epi32(b2, 19));


	// Round 3 
	a0 = _mm256_add_epi32(a0, _mm256_load_si256(block + 0));	a1 = _mm256_add_epi32(a1, _mm256_load_si256(block + 1));	a2 = _mm256_add_epi32(a2, _mm256_load_si256(block + 2));	t0 = _mm256_xor_si256(b0, c0);	t1 = _mm256_xor_si256(b1, c1);	t2 = _mm256_xor_si256(b2, c2);	a0 = _mm256_add_epi32(a0, _mm256_set1_epi32(0x6ed9eba1));	a1 = _mm256_add_epi32(a1, _mm256_set1_epi32(0x6ed9eba1));	a2 = _mm256_add_epi32(a2, _mm256_set1_epi32(0x6ed9eba1));	a0 = _mm256_add_epi32(a0, _mm256_xor_si256(t0, d0));	a1 = _mm256_add_epi32(a1, _mm256_xor_si256(t1, d1));	a2 = _mm256_add_epi32(a2, _mm256_xor_si256(t2, d2));	a0 = _mm256_or_si256(_mm256_slli_epi32(a0, 3), _mm256_srli_epi32(a0, 29));	a1 = _mm256_or_si256(_mm256_slli_epi32(a1, 3), _mm256_srli_epi32(a1, 29));	a2 = _mm256_or_si256(_mm256_slli_epi32(a2, 3), _mm256_srli_epi32(a2, 29));
	d0 = _mm256_add_epi32(d0, _mm256_load_si256(block + 24));	d1 = _mm256_add_epi32(d1, _mm256_load_si256(block + 25));	d2 = _mm256_add_epi32(d2, _mm256_load_si256(block + 26));	d0 = _mm256_add_epi32(d0, _mm256_set1_epi32(0x6ed9eba1));	d1 = _mm256_add_epi32(d1, _mm256_set1_epi32(0x6ed9eba1));	d2 = _mm256_add_epi32(d2, _mm256_set1_epi32(0x6ed9eba1));	d0 = _mm256_add_epi32(d0, _mm256_xor_si256(t0, a0));	d1 = _mm256_add_epi32(d1, _mm256_xor_si256(t1, a1));	d2 = _mm256_add_epi32(d2, _mm256_xor_si256(t2, a2));	d0 = _mm256_or_si256(_mm256_slli_epi32(d0, 9), _mm256_srli_epi32(d0, 23));	d1 = _mm256_or_si256(_mm256_slli_epi32(d1, 9), _mm256_srli_epi32(d1, 23));	d2 = _mm256_or_si256(_mm256_slli_epi32(d2, 9), _mm256_srli_epi32(d2, 23));
	c0 = _mm256_add_epi32(c0, _mm256_load_si256(block + 12));	c1 = _mm256_add_epi32(c1, _mm256_load_si256(block + 13));	c2 = _mm256_add_epi32(c2, _mm256_load_si256(block + 14));	t0 = _mm256_xor_si256(a0, d0);	t1 = _mm256_xor_si256(a1, d1);	t2 = _mm256_xor_si256(a2, d2);	c0 = _mm256_add_epi32(c0, _mm256_set1_epi32(0x6ed9eba1));	c1 = _mm256_add_epi32(c1, _mm256_set1_epi32(0x6ed9eba1));	c2 = _mm256_add_epi32(c2, _mm256_set1_epi32(0x6ed9eba1));	c0 = _mm256_add_epi32(c0, _mm256_xor_si256(t0, b0));	c1 = _mm256_add_epi32(c1, _mm256_xor_si256(t1, b1));	c2 = _mm256_add_epi32(c2, _mm256_xor_si256(t2, b2));	c0 = _mm256_or_si256(_mm256_slli_epi32(c0, 11), _mm256_srli_epi32(c0, 21));	c1 = _mm256_or_si256(_mm256_slli_epi32(c1, 11), _mm256_srli_epi32(c1, 21));	c2 = _mm256_or_si256(_mm256_slli_epi32(c2, 11), _mm256_srli_epi32(c2, 21));
	b0 = _mm256_add_epi32(b0, _mm256_load_si256(block + 36));	b1 = _mm256_add_epi32(b1, _mm256_load_si256(block + 37));	b2 = _mm256_add_epi32(b2, _mm256_load_si256(block + 38));	b0 = _mm256_add_epi32(b0, _mm256_set1_epi32(0x6ed9eba1));	b1 = _mm256_add_epi32(b1, _mm256_set1_epi32(0x6ed9eba1));	b2 = _mm256_add_epi32(b2, _mm256_set1_epi32(0x6ed9eba1));	b0 = _mm256_add_epi32(b0, _mm256_xor_si256(t0, c0));	b1 = _mm256_add_epi32(b1, _mm256_xor_si256(t1, c1));	b2 = _mm256_add_epi32(b2, _mm256_xor_si256(t2, c2));	b0 = _mm256_or_si256(_mm256_slli_epi32(b0, 15), _mm256_srli_epi32(b0, 17));	b1 = _mm256_or_si256(_mm256_slli_epi32(b1, 15), _mm256_srli_epi32(b1, 17));	b2 = _mm256_or_si256(_mm256_slli_epi32(b2, 15), _mm256_srli_epi32(b2, 17));
	a0 = _mm256_add_epi32(a0, _mm256_load_si256(block + 6));	a1 = _mm256_add_epi32(a1, _mm256_load_si256(block + 7));	a2 = _mm256_add_epi32(a2, _mm256_load_si256(block + 8));	t0 = _mm256_xor_si256(b0, c0);	t1 = _mm256_xor_si256(b1, c1);	t2 = _mm256_xor_si256(b2, c2);	a0 = _mm256_add_epi32(a0, _mm256_set1_epi32(0x6ed9eba1));	a1 = _mm256_add_epi32(a1, _mm256_set1_epi32(0x6ed9eba1));	a2 = _mm256_add_epi32(a2, _mm256_set1_epi32(0x6ed9eba1));	a0 = _mm256_add_epi32(a0, _mm256_xor_si256(t0, d0));	a1 = _mm256_add_epi32(a1, _mm256_xor_si256(t1, d1));	a2 = _mm256_add_epi32(a2, _mm256_xor_si256(t2, d2));	a0 = _mm256_or_si256(_mm256_slli_epi32(a0, 3), _mm256_srli_epi32(a0, 29));	a1 = _mm256_or_si256(_mm256_slli_epi32(a1, 3), _mm256_srli_epi32(a1, 29));	a2 = _mm256_or_si256(_mm256_slli_epi32(a2, 3), _mm256_srli_epi32(a2, 29));
	d0 = _mm256_add_epi32(d0, _mm256_load_si256(block + 30));	d1 = _mm256_add_epi32(d1, _mm256_load_si256(block + 31));	d2 = _mm256_add_epi32(d2, _mm256_load_si256(block + 32));	d0 = _mm256_add_epi32(d0, _mm256_set1_epi32(0x6ed9eba1));	d1 = _mm256_add_epi32(d1, _mm256_set1_epi32(0x6ed9eba1));	d2 = _mm256_add_epi32(d2, _mm256_set1_epi32(0x6ed9eba1));	d0 = _mm256_add_epi32(d0, _mm256_xor_si256(t0, a0));	d1 = _mm256_add_epi32(d1, _mm256_xor_si256(t1, a1));	d2 = _mm256_add_epi32(d2, _mm256_xor_si256(t2, a2));	d0 = _mm256_or_si256(_mm256_slli_epi32(d0, 9), _mm256_srli_epi32(d0, 23));	d1 = _mm256_or_si256(_mm256_slli_epi32(d1, 9), _mm256_srli_epi32(d1, 23));	d2 = _mm256_or_si256(_mm256_slli_epi32(d2, 9), _mm256_srli_epi32(d2, 23));
	c0 = _mm256_add_epi32(c0, _mm256_load_si256(block + 18));	c1 = _mm256_add_epi32(c1, _mm256_load_si256(block + 19));	c2 = _mm256_add_epi32(c2, _mm256_load_si256(block + 20));	t0 = _mm256_xor_si256(a0, d0);	t1 = _mm256_xor_si256(a1, d1);	t2 = _mm256_xor_si256(a2, d2);	c0 = _mm256_add_epi32(c0, _mm256_set1_epi32(0x6ed9eba1));	c1 = _mm256_add_epi32(c1, _mm256_set1_epi32(0x6ed9eba1));	c2 = _mm256_add_epi32(c2, _mm256_set1_epi32(0x6ed9eba1));	c0 = _mm256_add_epi32(c0, _mm256_xor_si256(t0, b0));	c1 = _mm256_add_epi32(c1, _mm256_xor_si256(t1, b1));	c2 = _mm256_add_epi32(c2, _mm256_xor_si256(t2, b2));	c0 = _mm256_or_si256(_mm256_slli_epi32(c0, 11), _mm256_srli_epi32(c0, 21));	c1 = _mm256_or_si256(_mm256_slli_epi32(c1, 11), _mm256_srli_epi32(c1, 21));	c2 = _mm256_or_si256(_mm256_slli_epi32(c2, 11), _mm256_srli_epi32(c2, 21));
	b0 = _mm256_add_epi32(b0, _mm256_load_si256(block + 42));	b1 = _mm256_add_epi32(b1, _mm256_load_si256(block + 43));	b2 = _mm256_add_epi32(b2, _mm256_load_si256(block + 44));	b0 = _mm256_add_epi32(b0, _mm256_set1_epi32(0x6ed9eba1));	b1 = _mm256_add_epi32(b1, _mm256_set1_epi32(0x6ed9eba1));	b2 = _mm256_add_epi32(b2, _mm256_set1_epi32(0x6ed9eba1));	b0 = _mm256_add_epi32(b0, _mm256_xor_si256(t0, c0));	b1 = _mm256_add_epi32(b1, _mm256_xor_si256(t1, c1));	b2 = _mm256_add_epi32(b2, _mm256_xor_si256(t2, c2));	b0 = _mm256_or_si256(_mm256_slli_epi32(b0, 15), _mm256_srli_epi32(b0, 17));	b1 = _mm256_or_si256(_mm256_slli_epi32(b1, 15), _mm256_srli_epi32(b1, 17));	b2 = _mm256_or_si256(_mm256_slli_epi32(b2, 15), _mm256_srli_epi32(b2, 17));
	a0 = _mm256_add_epi32(a0, _mm256_load_si256(block + 3));	a1 = _mm256_add_epi32(a1, _mm256_load_si256(block + 4));	a2 = _mm256_add_epi32(a2, _mm256_load_si256(block + 5));	t0 = _mm256_xor_si256(b0, c0);	t1 = _mm256_xor_si256(b1, c1);	t2 = _mm256_xor_si256(b2, c2);	a0 = _mm256_add_epi32(a0, _mm256_set1_epi32(0x6ed9eba1));	a1 = _mm256_add_epi32(a1, _mm256_set1_epi32(0x6ed9eba1));	a2 = _mm256_add_epi32(a2, _mm256_set1_epi32(0x6ed9eba1));	a0 = _mm256_add_epi32(a0, _mm256_xor_si256(t0, d0));	a1 = _mm256_add_epi32(a1, _mm256_xor_si256(t1, d1));	a2 = _mm256_add_epi32(a2, _mm256_xor_si256(t2, d2));	a0 = _mm256_or_si256(_mm256_slli_epi32(a0, 3), _mm256_srli_epi32(a0, 29));	a1 = _mm256_or_si256(_mm256_slli_epi32(a1, 3), _mm256_srli_epi32(a1, 29));	a2 = _mm256_or_si256(_mm256_slli_epi32(a2, 3), _mm256_srli_epi32(a2, 29));
	d0 = _mm256_add_epi32(d0, _mm256_load_si256(block + 27));	d1 = _mm256_add_epi32(d1, _mm256_load_si256(block + 28));	d2 = _mm256_add_epi32(d2, _mm256_load_si256(block + 29));	d0 = _mm256_add_epi32(d0, _mm256_set1_epi32(0x6ed9eba1));	d1 = _mm256_add_epi32(d1, _mm256_set1_epi32(0x6ed9eba1));	d2 = _mm256_add_epi32(d2, _mm256_set1_epi32(0x6ed9eba1));	d0 = _mm256_add_epi32(d0, _mm256_xor_si256(t0, a0));	d1 = _mm256_add_epi32(d1, _mm256_xor_si256(t1, a1));	d2 = _mm256_add_epi32(d2, _mm256_xor_si256(t2, a2));	d0 = _mm256_or_si256(_mm256_slli_epi32(d0, 9), _mm256_srli_epi32(d0, 23));	d1 = _mm256_or_si256(_mm256_slli_epi32(d1, 9), _mm256_srli_epi32(d1, 23));	d2 = _mm256_or_si256(_mm256_slli_epi32(d2, 9), _mm256_srli_epi32(d2, 23));
	c0 = _mm256_add_epi32(c0, _mm256_load_si256(block + 15));	c1 = _mm256_add_epi32(c1, _mm256_load_si256(block + 16));	c2 = _mm256_add_epi32(c2, _mm256_load_si256(block + 17));	t0 = _mm256_xor_si256(a0, d0);	t1 = _mm256_xor_si256(a1, d1);	t2 = _mm256_xor_si256(a2, d2);	c0 = _mm256_add_epi32(c0, _mm256_set1_epi32(0x6ed9eba1));	c1 = _mm256_add_epi32(c1, _mm256_set1_epi32(0x6ed9eba1));	c2 = _mm256_add_epi32(c2, _mm256_set1_epi32(0x6ed9eba1));	c0 = _mm256_add_epi32(c0, _mm256_xor_si256(t0, b0));	c1 = _mm256_add_epi32(c1, _mm256_xor_si256(t1, b1));	c2 = _mm256_add_epi32(c2, _mm256_xor_si256(t2, b2));	c0 = _mm256_or_si256(_mm256_slli_epi32(c0, 11), _mm256_srli_epi32(c0, 21));	c1 = _mm256_or_si256(_mm256_slli_epi32(c1, 11), _mm256_srli_epi32(c1, 21));	c2 = _mm256_or_si256(_mm256_slli_epi32(c2, 11), _mm256_srli_epi32(c2, 21));
	b0 = _mm256_add_epi32(b0, _mm256_load_si256(block + 39));	b1 = _mm256_add_epi32(b1, _mm256_load_si256(block + 40));	b2 = _mm256_add_epi32(b2, _mm256_load_si256(block + 41));	b0 = _mm256_add_epi32(b0, _mm256_set1_epi32(0x6ed9eba1));	b1 = _mm256_add_epi32(b1, _mm256_set1_epi32(0x6ed9eba1));	b2 = _mm256_add_epi32(b2, _mm256_set1_epi32(0x6ed9eba1));	b0 = _mm256_add_epi32(b0, _mm256_xor_si256(t0, c0));	b1 = _mm256_add_epi32(b1, _mm256_xor_si256(t1, c1));	b2 = _mm256_add_epi32(b2, _mm256_xor_si256(t2, c2));	b0 = _mm256_or_si256(_mm256_slli_epi32(b0, 15), _mm256_srli_epi32(b0, 17));	b1 = _mm256_or_si256(_mm256_slli_epi32(b1, 15), _mm256_srli_epi32(b1, 17));	b2 = _mm256_or_si256(_mm256_slli_epi32(b2, 15), _mm256_srli_epi32(b2, 17));
	a0 = _mm256_add_epi32(a0, _mm256_load_si256(block + 9));	a1 = _mm256_add_epi32(a1, _mm256_load_si256(block + 10));	a2 = _mm256_add_epi32(a2, _mm256_load_si256(block + 11));	t0 = _mm256_xor_si256(b0, c0);	t1 = _mm256_xor_si256(b1, c1);	t2 = _mm256_xor_si256(b2, c2);	a0 = _mm256_add_epi32(a0, _mm256_set1_epi32(0x6ed9eba1));	a1 = _mm256_add_epi32(a1, _mm256_set1_epi32(0x6ed9eba1));	a2 = _mm256_add_epi32(a2, _mm256_set1_epi32(0x6ed9eba1));	a0 = _mm256_add_epi32(a0, _mm256_xor_si256(t0, d0));	a1 = _mm256_add_epi32(a1, _mm256_xor_si256(t1, d1));	a2 = _mm256_add_epi32(a2, _mm256_xor_si256(t2, d2));	a0 = _mm256_or_si256(_mm256_slli_epi32(a0, 3), _mm256_srli_epi32(a0, 29));	a1 = _mm256_or_si256(_mm256_slli_epi32(a1, 3), _mm256_srli_epi32(a1, 29));	a2 = _mm256_or_si256(_mm256_slli_epi32(a2, 3), _mm256_srli_epi32(a2, 29));
	d0 = _mm256_add_epi32(d0, _mm256_load_si256(block + 33));	d1 = _mm256_add_epi32(d1, _mm256_load_si256(block + 34));	d2 = _mm256_add_epi32(d2, _mm256_load_si256(block + 35));	d0 = _mm256_add_epi32(d0, _mm256_set1_epi32(0x6ed9eba1));	d1 = _mm256_add_epi32(d1, _mm256_set1_epi32(0x6ed9eba1));	d2 = _mm256_add_epi32(d2, _mm256_set1_epi32(0x6ed9eba1));	d0 = _mm256_add_epi32(d0, _mm256_xor_si256(t0, a0));	d1 = _mm256_add_epi32(d1, _mm256_xor_si256(t1, a1));	d2 = _mm256_add_epi32(d2, _mm256_xor_si256(t2, a2));	d0 = _mm256_or_si256(_mm256_slli_epi32(d0, 9), _mm256_srli_epi32(d0, 23));	d1 = _mm256_or_si256(_mm256_slli_epi32(d1, 9), _mm256_srli_epi32(d1, 23));	d2 = _mm256_or_si256(_mm256_slli_epi32(d2, 9), _mm256_srli_epi32(d2, 23));
	c0 = _mm256_add_epi32(c0, _mm256_load_si256(block + 21));	c1 = _mm256_add_epi32(c1, _mm256_load_si256(block + 22));	c2 = _mm256_add_epi32(c2, _mm256_load_si256(block + 23));	t0 = _mm256_xor_si256(a0, d0);	t1 = _mm256_xor_si256(a1, d1);	t2 = _mm256_xor_si256(a2, d2);	c0 = _mm256_add_epi32(c0, _mm256_set1_epi32(0x6ed9eba1));	c1 = _mm256_add_epi32(c1, _mm256_set1_epi32(0x6ed9eba1));	c2 = _mm256_add_epi32(c2, _mm256_set1_epi32(0x6ed9eba1));	c0 = _mm256_add_epi32(c0, _mm256_xor_si256(t0, b0));	c1 = _mm256_add_epi32(c1, _mm256_xor_si256(t1, b1));	c2 = _mm256_add_epi32(c2, _mm256_xor_si256(t2, b2));	c0 = _mm256_or_si256(_mm256_slli_epi32(c0, 11), _mm256_srli_epi32(c0, 21));	c1 = _mm256_or_si256(_mm256_slli_epi32(c1, 11), _mm256_srli_epi32(c1, 21));	c2 = _mm256_or_si256(_mm256_slli_epi32(c2, 11), _mm256_srli_epi32(c2, 21));
	b0 = _mm256_add_epi32(b0, _mm256_load_si256(block + 45));	b1 = _mm256_add_epi32(b1, _mm256_load_si256(block + 46));	b2 = _mm256_add_epi32(b2, _mm256_load_si256(block + 47));	b0 = _mm256_add_epi32(b0, _mm256_set1_epi32(0x6ed9eba1));	b1 = _mm256_add_epi32(b1, _mm256_set1_epi32(0x6ed9eba1));	b2 = _mm256_add_epi32(b2, _mm256_set1_epi32(0x6ed9eba1));	b0 = _mm256_add_epi32(b0, _mm256_xor_si256(t0, c0));	b1 = _mm256_add_epi32(b1, _mm256_xor_si256(t1, c1));	b2 = _mm256_add_epi32(b2, _mm256_xor_si256(t2, c2));	b0 = _mm256_or_si256(_mm256_slli_epi32(b0, 15), _mm256_srli_epi32(b0, 17));	b1 = _mm256_or_si256(_mm256_slli_epi32(b1, 15), _mm256_srli_epi32(b1, 17));	b2 = _mm256_or_si256(_mm256_slli_epi32(b2, 15), _mm256_srli_epi32(b2, 17));

	_mm256_store_si256(state + 0, _mm256_add_epi32(_mm256_load_si256(state + 0), a0));	_mm256_store_si256(state + 1, _mm256_add_epi32(_mm256_load_si256(state + 1), a1));	_mm256_store_si256(state + 2, _mm256_add_epi32(_mm256_load_si256(state + 2), a2));
	_mm256_store_si256(state + 3, _mm256_add_epi32(_mm256_load_si256(state + 3), b0));	_mm256_store_si256(state + 4, _mm256_add_epi32(_mm256_load_si256(state + 4), b1));	_mm256_store_si256(state + 5, _mm256_add_epi32(_mm256_load_si256(state + 5), b2));
	_mm256_store_si256(state + 6, _mm256_add_epi32(_mm256_load_si256(state + 6), c0));	_mm256_store_si256(state + 7, _mm256_add_epi32(_mm256_load_si256(state + 7), c1));	_mm256_store_si256(state + 8, _mm256_add_epi32(_mm256_load_si256(state + 8), c2));
	_mm256_store_si256(state + 9, _mm256_add_epi32(_mm256_load_si256(state + 9), d0));	_mm256_store_si256(state + 10, _mm256_add_epi32(_mm256_load_si256(state + 10), d1));	_mm256_store_si256(state + 11, _mm256_add_epi32(_mm256_load_si256(state + 11), d2));
}

